<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://192.168.1.77:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://192.168.1.77:4000/" rel="alternate" type="text/html" /><updated>2018-07-20T22:21:09-04:00</updated><id>http://192.168.1.77:4000/</id><title type="html">Linux in EnterPrise</title><subtitle>Focus in Linux, LAMP, Mail, Cluster Arch(ENterprise Linux Apache MySQL Php)</subtitle><entry><title type="html">Migrated from WP to Jekyll!</title><link href="http://192.168.1.77:4000/jekyll/update/2018/07/20/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Migrated from WP to Jekyll!" /><published>2018-07-20T21:33:54-04:00</published><updated>2018-07-20T21:33:54-04:00</updated><id>http://192.168.1.77:4000/jekyll/update/2018/07/20/welcome-to-jekyll</id><content type="html" xml:base="http://192.168.1.77:4000/jekyll/update/2018/07/20/welcome-to-jekyll.html">&lt;p&gt;For who reach my old site. I just migrated from Wordpress to Jekyll. Haven’t update this site for a while. But don’t want get lost notes I made that may help someone. so I migrated to Jekyll just for record purpose.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">For who reach my old site. I just migrated from Wordpress to Jekyll. Haven’t update this site for a while. But don’t want get lost notes I made that may help someone. so I migrated to Jekyll just for record purpose.</summary></entry><entry><title type="html">when openssh will kick out idle users ?</title><link href="http://192.168.1.77:4000/2016-05-when-openssh-will-kick-out-idle-users/" rel="alternate" type="text/html" title="when openssh will kick out idle users ?" /><published>2016-05-31T17:05:35-04:00</published><updated>2016-05-31T17:05:35-04:00</updated><id>http://192.168.1.77:4000/when-openssh-will-kick-out-idle-users</id><content type="html" xml:base="http://192.168.1.77:4000/2016-05-when-openssh-will-kick-out-idle-users/">&lt;p&gt;when reading CIS security baseline. it mentions following lines:&lt;/p&gt;

&lt;p&gt;Having no timeout value associated with a connection could allow an unauthorized user access to another user’s ssh session (e.g. user walks away from their computer and doesn’t lock the screen). Setting a timeout value at least reduces the risk of this happening..&lt;/p&gt;

&lt;p&gt;While the recommended setting is 300 seconds (5 minutes), set this timeout value based on site policy. The recommended setting for ClientAliveCountMax is 0. In this case, the client session will be terminated after 5 minutes of idle time and no keepalive messages will be sent.&lt;/p&gt;

&lt;p&gt;Review our settings:&lt;/p&gt;

&lt;p&gt;ClientAliveInterval 300&lt;/p&gt;

&lt;p&gt;ClientAliveCountMax 1&lt;/p&gt;

&lt;p&gt;According to man sshd_config&lt;/p&gt;

&lt;p&gt;If ClientAliveInterval (see below) is set to 15, and ClientAliveCountMax is left at the default, unresponsive SSH clients will be disconnected after approximately 45 seconds. This option applies to protocol version 2 only.&lt;/p&gt;

&lt;p&gt;Interesting thing is: you won’t be kicked out after 45s if you set as above with Protocol 2.&lt;/p&gt;

&lt;p&gt;From my test: the timeout will ONLY work when you set ClientAliveCountMax to 0. and idle time set to what you want kick out the user.&lt;/p&gt;</content><author><name>edyliu</name></author><category term="ssh" /><summary type="html">when reading CIS security baseline. it mentions following lines:</summary></entry><entry><title type="html">(Fixed) I/O error trying to sync with MASTER: connection lost</title><link href="http://192.168.1.77:4000/2015-08-fixed-io-error-trying-to-sync-with-master-connection-lost/" rel="alternate" type="text/html" title="(Fixed) I/O error trying to sync with MASTER: connection lost" /><published>2015-08-25T21:05:37-04:00</published><updated>2015-08-25T21:05:37-04:00</updated><id>http://192.168.1.77:4000/fixed-io-error-trying-to-sync-with-master-connection-lost</id><content type="html" xml:base="http://192.168.1.77:4000/2015-08-fixed-io-error-trying-to-sync-with-master-connection-lost/">&lt;p&gt;Error:&lt;/p&gt;

&lt;p&gt;[2162] 26 Aug 15:26:26.795 # I/O error trying to sync with MASTER: connection lost&lt;/p&gt;

&lt;p&gt;Above error happened when there is poor network connection, and you do the slave sync first time.&lt;/p&gt;

&lt;p&gt;Possible Solution: increase slave buffer from 256m 64m 60 to 512M 128m 120&lt;/p&gt;

&lt;p&gt;x.x.x.x:6379&amp;gt; config get client-output-buffer-limit&lt;/p&gt;

&lt;p&gt;1) “client-output-buffer-limit”&lt;/p&gt;

&lt;p&gt;2) “normal 0 0 0 slave 268435456 67108864 60 pubsub 33554432 8388608 60”&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;x.x.x.x:6379&amp;gt; config set client-output-buffer-limit “normal 0 0 0 slave 536870912 134217728 120 pubsub 33554432 8388608 60”&lt;/p&gt;

&lt;p&gt;x.x.x.x:6379&amp;gt; config get client-output-buffer-limit&lt;/p&gt;

&lt;p&gt;2) “normal 0 0 0 slave 536870912 134217728 120 pubsub 33554432 8388608 60”&lt;/p&gt;

&lt;p&gt;Once your first sync completed. you can rollback the setting.&lt;/p&gt;

&lt;p&gt;Refer: http://redis.io/topics/clients&lt;/p&gt;</content><author><name>edyliu</name></author><category term="redis" /><summary type="html">Error:</summary></entry><entry><title type="html">Install ZeroMQ plugin for MySQL</title><link href="http://192.168.1.77:4000/2015-08-install-zeromq-plugin-for-mysql/" rel="alternate" type="text/html" title="Install ZeroMQ plugin for MySQL" /><published>2015-08-24T22:28:24-04:00</published><updated>2015-08-24T22:28:24-04:00</updated><id>http://192.168.1.77:4000/install-zeromq-plugin-for-mysql</id><content type="html" xml:base="http://192.168.1.77:4000/2015-08-install-zeromq-plugin-for-mysql/">&lt;p&gt;Following steps in CentOS 6.x&lt;/p&gt;

&lt;p&gt;Download cmake , use the binary one. such as: cmake-3.2.3-Linux-x86_64.sh&lt;/p&gt;

&lt;p&gt;http://www.cmake.org/download/&lt;/p&gt;

&lt;p&gt;Install Cmake&lt;/p&gt;

&lt;p&gt;chmod +x cmake-3.2.3-Linux-x86_64.sh&lt;/p&gt;

&lt;p&gt;./cmake-3.2.3-Linux-x86_64.sh&lt;/p&gt;

&lt;p&gt;Get mysql zeromq plugin:&lt;/p&gt;

&lt;p&gt;git clone https://github.com/netkiller/mysql-zmq-plugin.git&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Update headers:&lt;/p&gt;

&lt;p&gt;cd mysql-zmq-plugin&lt;/p&gt;

&lt;p&gt;vi CMakeLists.txt , replace with your own headers&lt;/p&gt;

&lt;p&gt;”INCLUDE_DIRECTORIES(/usr/local/mysql-xxx/include)”&lt;/p&gt;

&lt;p&gt;”INCLUDE_DIRECTORIES(/usr/local/zeromq-xxx/include)”&lt;/p&gt;

&lt;p&gt;cmake .&lt;/p&gt;

&lt;p&gt;make&lt;/p&gt;

&lt;p&gt;Then you will get libzeromq.so&lt;/p&gt;

&lt;p&gt;sudo cp libzeromq.so /path/to/mysql/plugin&lt;/p&gt;

&lt;p&gt;Refer:&lt;/p&gt;

&lt;p&gt;http://zeromq.org/event:zeromq-for-mysql&lt;/p&gt;

&lt;p&gt;Known issue:&lt;/p&gt;

&lt;p&gt;If you got following complain:&lt;/p&gt;

&lt;pre&gt;/usr/bin/ld: cannot find -lzmq
collect2: ld returned 1 exit status
make[2]: *** [libzeromq.so] Error 1
make[1]: *** [CMakeFiles/zeromq.dir/all] Error 2
&lt;/pre&gt;

&lt;p&gt;you can&lt;/p&gt;

&lt;p&gt;ln -s your_zmq_so to /lib64/libzmq.so&lt;/p&gt;</content><author><name>edyliu</name></author><category term="mysql" /><summary type="html">Following steps in CentOS 6.x</summary></entry><entry><title type="html">php access hive2 server by thrift</title><link href="http://192.168.1.77:4000/2015-08-php-access-hive2-server-by-thrift/" rel="alternate" type="text/html" title="php access hive2 server by thrift" /><published>2015-08-17T20:31:21-04:00</published><updated>2015-08-17T20:31:21-04:00</updated><id>http://192.168.1.77:4000/php-access-hive2-server-by-thrift</id><content type="html" xml:base="http://192.168.1.77:4000/2015-08-php-access-hive2-server-by-thrift/">&lt;p&gt;You can get some guys did on https://github.com/search?utf8=%E2%9C%93&amp;amp;q=php+thrift+hive&lt;/p&gt;

&lt;p&gt;When you try to access Hive2 without SASL. you may met following alerts:&lt;/p&gt;

&lt;p&gt;TTransportException’ with message ‘TSocket: timed out reading 67108844 bytes from&lt;/p&gt;

&lt;p&gt;Try add following lines to /etc/hive/conf/hive-site.xml&lt;/p&gt;

&lt;blockquote&gt;
  &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt;
&lt;/property&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;value&gt;NOSASL&lt;/value&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;description&gt;Client authentication types.
&lt;/description&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;NONE: no authentication check&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;LDAP: LDAP/AD based authentication&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;KERBEROS: Kerberos/GSSAPI authentication&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;CUSTOM: Custom authentication provider&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(Use with property hive.server2.custom.authentication.class)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&amp;lt;/description&amp;gt; &amp;lt;/property&amp;gt;
Notes: NOSASL not equal NONE. NONE is default and means Plain SASL.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>edyliu</name></author><summary type="html">You can get some guys did on https://github.com/search?utf8=%E2%9C%93&amp;amp;q=php+thrift+hive</summary></entry><entry><title type="html">(fixed) sqoop import from Oracle to Hive throw Heap size error</title><link href="http://192.168.1.77:4000/2015-08-fixed-sqoop-import-from-oracle-to-hive-throw-heap-size-error/" rel="alternate" type="text/html" title="(fixed) sqoop import from Oracle to Hive throw Heap size error" /><published>2015-08-06T20:43:01-04:00</published><updated>2015-08-06T20:43:01-04:00</updated><id>http://192.168.1.77:4000/fixed-sqoop-import-from-oracle-to-hive-throw-heap-size-error</id><content type="html" xml:base="http://192.168.1.77:4000/2015-08-fixed-sqoop-import-from-oracle-to-hive-throw-heap-size-error/">&lt;p&gt;If you use CDH and Sqoop. you probably met following issue:&lt;/p&gt;

&lt;p&gt;15/08/07 15:21:55 INFO manager.SqlManager: Using default fetchSize of 1000&lt;/p&gt;

&lt;p&gt;15/08/07 15:21:55 INFO tool.CodeGenTool: Beginning code generation&lt;/p&gt;

&lt;p&gt;15/08/07 15:21:56 INFO manager.OracleManager: Time zone has been set to GMT&lt;/p&gt;

&lt;p&gt;15/08/07 15:21:56 INFO manager.SqlManager: Executing SQL statement: select xxx where (1 = 0)&lt;/p&gt;

&lt;p&gt;Exception in thread “main” java.lang.OutOfMemoryError: Java heap space&lt;/p&gt;

&lt;p&gt;at java.lang.reflect.Array.newArray(Native Method)&lt;/p&gt;

&lt;p&gt;at java.lang.reflect.Array.newInstance(Array.java:70)&lt;/p&gt;

&lt;p&gt;at oracle.jdbc.driver.BufferCache.get(BufferCache.java:226)&lt;/p&gt;

&lt;p&gt;at oracle.jdbc.driver.PhysicalConnection.getCharBuffer(PhysicalConnection.java:7698)&lt;/p&gt;

&lt;p&gt;at oracle.jdbc.driver.OracleStatement.prepareAccessors(OracleStatement.java:1013)&lt;/p&gt;

&lt;p&gt;at oracle.jdbc.driver.T4CTTIdcb.receiveCommon(T4CTTIdcb.java:277)&lt;/p&gt;

&lt;p&gt;This related with HDFS client HEAP settings. you can fix it by increase HEAP size for HDFS client.&lt;/p&gt;

&lt;p&gt;from CDH:&lt;/p&gt;

&lt;p&gt;Client Java Heap Size in Bytes.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;or manually update hadoop-env.sh. default is 256M. you can increase -Xmx to whatever you want.&lt;/p&gt;

&lt;p&gt;diff /etc/hadoop/conf/hadoop-env.sh /etc/hadoop/conf/hadoop-env.shold&lt;/p&gt;

&lt;p&gt;61c61&lt;/p&gt;

&lt;p&gt;&amp;lt; export HADOOP_CLIENT_OPTS=”-Xms268435456 -Xmx1268435456 -Djava.net.preferIPv4Stack=true $HADOOP_CLIENT_OPTS” -– &amp;gt; export HADOOP_CLIENT_OPTS=”-Xms268435456 -Xmx268435456 -Djava.net.preferIPv4Stack=true $HADOOP_CLIENT_OPTS”&lt;/p&gt;</content><author><name>edyliu</name></author><category term="bigdata" /><summary type="html">If you use CDH and Sqoop. you probably met following issue:</summary></entry><entry><title type="html">OS Tuning tips for Hadoop Cluster</title><link href="http://192.168.1.77:4000/2015-07-os-tuning-tips-for-hadoop-cluster/" rel="alternate" type="text/html" title="OS Tuning tips for Hadoop Cluster" /><published>2015-07-09T14:36:17-04:00</published><updated>2015-07-09T14:36:17-04:00</updated><id>http://192.168.1.77:4000/os-tuning-tips-for-hadoop-cluster</id><content type="html" xml:base="http://192.168.1.77:4000/2015-07-os-tuning-tips-for-hadoop-cluster/">&lt;p&gt;1.Decrease swappiness.&lt;/p&gt;

&lt;p&gt;Reason:&lt;/p&gt;

&lt;p&gt;A value from 0 to 100 which controls the degree to which the system swaps. A high value prioritizes system performance, aggressively swapping processes out of physical memory when they are not active. A low value prioritizes interactivity and avoids swapping processes out of physical memory for as long as possible, which decreases response latency. The default value is 60.&lt;/p&gt;

&lt;p&gt;Default value: 60&lt;/p&gt;

&lt;p&gt;Recommend value: 5&lt;/p&gt;

&lt;p&gt;Online Change: Y&lt;/p&gt;

&lt;p&gt;Action:&lt;/p&gt;

&lt;p&gt;# update online&lt;/p&gt;

&lt;p&gt;echo 5 &amp;gt; /proc/sys/vm/swappiness&lt;/p&gt;

&lt;p&gt;# update permanently , edit /etc/sysctl.conf and add following line:&lt;/p&gt;

&lt;p&gt;vm.swappiness = 5&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;2.Filesystem remount with noatime&lt;/p&gt;

&lt;p&gt;Reason:&lt;/p&gt;

&lt;p&gt;Ext4 default mount with atime, means the IO will update the filesystem with access time. Will bring extral IO to disk, atime useless to your HDFS filesystem, while noatime, means write operation will not update inode access times on this file system.&lt;/p&gt;

&lt;p&gt;Default value: atime&lt;/p&gt;

&lt;p&gt;Recommend value: noatime&lt;/p&gt;

&lt;p&gt;Online Change: Y&lt;/p&gt;

&lt;p&gt;Action:&lt;/p&gt;

&lt;p&gt;# update online&lt;/p&gt;

&lt;p&gt;mount -o remount,noatime /dataX&lt;/p&gt;

&lt;p&gt;# update permanently , edit /etc/fstab&lt;/p&gt;

&lt;p&gt;/dev/sdxx /dataX ext4 defaults,noatime 1 1&lt;/p&gt;

&lt;p&gt;3.Turn off reserved disk for HDFS disks&lt;/p&gt;

&lt;p&gt;Reason:&lt;/p&gt;

&lt;p&gt;As a security measure the ext3 file system reserves 5% of device space for administrative processes. This protects the system by allowing root processes to continue using the disk if a user process runs wild and fills it up. With today’s larger disk capacities, 5% equates into gigabytes of arguably wasted space. This is unnecessary for HDFS because it has 3 replica. It’s safe to disable it.Based on our current disks. 0.2Tx30x6 = 36T. We are wasting 36T space.&lt;/p&gt;

&lt;p&gt;Default value: 5%&lt;/p&gt;

&lt;p&gt;Recommend value: 0%&lt;/p&gt;

&lt;p&gt;Online Change: Y&lt;/p&gt;

&lt;p&gt;Action:&lt;/p&gt;

&lt;p&gt;# this online change, no impact. Note: only do following on HDFS data disks.&lt;/p&gt;

&lt;p&gt;sudo tune2fs -m 0 /dev/sdcX&lt;/p&gt;

&lt;p&gt;tune2fs 1.41.12 (17-May-2010)&lt;/p&gt;

&lt;p&gt;Setting reserved blocks percentage to 0% (0 blocks)&lt;/p&gt;

&lt;p&gt;4.Disable transparent hugepage(THP) compaction&lt;/p&gt;

&lt;p&gt;Reason:&lt;/p&gt;

&lt;p&gt;Most Linux platforms supported by CDH 5 include a feature called transparent hugepage compaction which interacts poorly with Hadoop workloads and can seriously degrade performance.&lt;/p&gt;

&lt;p&gt;Default value: enabled&lt;/p&gt;

&lt;p&gt;Recommend value: disable&lt;/p&gt;

&lt;p&gt;Online Change: Y&lt;/p&gt;

&lt;p&gt;Action:&lt;/p&gt;

&lt;p&gt;# online update&lt;/p&gt;

&lt;p&gt;echo never &amp;gt; /sys/kernel/mm/transparent_hugepage/enabled&lt;/p&gt;

&lt;p&gt;echo never &amp;gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag&lt;/p&gt;

&lt;p&gt;# update permanently , add following lines to /etc/rc.local&lt;/p&gt;

&lt;p&gt;echo never &amp;gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled&lt;/p&gt;

&lt;p&gt;echo never &amp;gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag&lt;/p&gt;

&lt;p&gt;5.Vitual Memory tuning.&lt;/p&gt;

&lt;p&gt;Reason:&lt;/p&gt;

&lt;p&gt;OS default use overcommit ratio to 50%, means we have some waste of our RAM.&lt;/p&gt;

&lt;p&gt;Default value: 50&lt;/p&gt;

&lt;p&gt;Recommend value: 95&lt;/p&gt;

&lt;p&gt;Online Change: Y&lt;/p&gt;

&lt;p&gt;Action:&lt;/p&gt;

&lt;p&gt;# online update&lt;/p&gt;

&lt;p&gt;echo 95 &amp;gt; /proc/sys/vm/overcommit_ratio&lt;/p&gt;

&lt;p&gt;echo 2 &amp;gt; /proc/sys/vm/overcommit_memory&lt;/p&gt;

&lt;p&gt;# update permanently , add following lines to /etc/sysctl.conf&lt;/p&gt;

&lt;p&gt;vm.overcommit_ratio=95&lt;/p&gt;

&lt;p&gt;vm.overcommit_memory=2&lt;/p&gt;

&lt;p&gt;Refer:&lt;/p&gt;

&lt;p&gt;http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_admin_performance.html&lt;/p&gt;

&lt;p&gt;http://www.slideshare.net/technmsg/improving-hadoop-cluster-performance-via-linux-configuration&lt;/p&gt;

&lt;p&gt;https://iainvlinux.wordpress.com/2014/02/16/memory-overcommit-settings/&lt;/p&gt;</content><author><name>edyliu</name></author><category term="hadoop" /><summary type="html">1.Decrease swappiness. Reason: A value from 0 to 100 which controls the degree to which the system swaps. A high value prioritizes system performance, aggressively swapping processes out of physical memory when they are not active. A low value prioritizes interactivity and avoids swapping processes out of physical memory for as long as possible, which decreases response latency. The default value is 60.</summary></entry><entry><title type="html">Migrate existing hadoop to CDH</title><link href="http://192.168.1.77:4000/2015-07-migrate-existing-hadoop-to-cdh/" rel="alternate" type="text/html" title="Migrate existing hadoop to CDH" /><published>2015-07-07T19:09:59-04:00</published><updated>2015-07-07T19:09:59-04:00</updated><id>http://192.168.1.77:4000/migrate-existing-hadoop-to-cdh</id><content type="html" xml:base="http://192.168.1.77:4000/2015-07-migrate-existing-hadoop-to-cdh/">&lt;p&gt;Don’t need to sell CDH’s benefits. you should know it before want to migrate 🙂&lt;/p&gt;

&lt;p&gt;Very Important, The following has been tested in my lab, all goes fine. can’t grantee if also works for you.&lt;/p&gt;

&lt;p&gt;I migrate from Apache Hadoop 2.2 to CDH 5.3 or 5.4 all works.&lt;/p&gt;

&lt;p&gt;## Backup namenode&lt;/p&gt;

&lt;p&gt;# cd /mnt/hadoop/hdfs/name&lt;/p&gt;

&lt;p&gt;# tar -cvf /root/nn_backup_data.tar .&lt;/p&gt;

&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;./current/fsimage&lt;/p&gt;

&lt;p&gt;..&lt;/p&gt;

&lt;p&gt;./current/edits&lt;/p&gt;

&lt;p&gt;./image/&lt;/p&gt;

&lt;p&gt;./image/fsimage&lt;/p&gt;

&lt;p&gt;## Install CDH WITHOUT create any service.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;## Create HDFS service.&lt;/p&gt;

&lt;p&gt;Important: your NN or DN data directory must match existing Hadoop. don’t worry, CDH won’t format/clean your existing data.&lt;/p&gt;

&lt;p&gt;## Stop existing Hadoop, do HDFS metadata upgrade.&lt;/p&gt;

&lt;p&gt;Option1: Cloudera Manager GUI.&lt;/p&gt;

&lt;p&gt;HDFS-&amp;gt;Upgrade HDFS Metadata-&amp;gt; Finalize Metadata Upgrade&lt;/p&gt;

&lt;p&gt;Option2: login NN.&lt;/p&gt;

&lt;p&gt;hdfs namenode -upgrade&lt;/p&gt;

&lt;p&gt;## Start HDFS from CM GUI now.&lt;/p&gt;

&lt;p&gt;Issue you may meet during Migration:&lt;/p&gt;

&lt;p&gt;Issue1:&lt;/p&gt;

&lt;p&gt;2015-07-23 13:54:04,150 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.&lt;/p&gt;

&lt;p&gt;org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /data/hadoop/hdfs/name is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:349)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:356)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:376)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:268)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1061)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:765)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:584)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:643)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:810)&lt;/init&gt;&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:794)&lt;/init&gt;&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1487)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1553)&lt;/p&gt;

&lt;p&gt;2015-07-23 13:54:04,152 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1&lt;/p&gt;

&lt;p&gt;2015-07-23 13:54:04,179 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:&lt;/p&gt;

&lt;p&gt;# This may caused by Last upgrade didn’t complete.&lt;/p&gt;

&lt;p&gt;hdfs dfsadmin -finalizeUpgrade&lt;/p&gt;

&lt;p&gt;# Issue2 : This may happened when you do HA for HDFS.&lt;/p&gt;

&lt;p&gt;2015-07-23 14:28:16,082 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@master.xxxx:50070&lt;/p&gt;

&lt;p&gt;2015-07-23 14:28:16,183 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system…&lt;/p&gt;

&lt;p&gt;2015-07-23 14:28:16,183 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.&lt;/p&gt;

&lt;p&gt;2015-07-23 14:28:16,183 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.&lt;/p&gt;

&lt;p&gt;2015-07-23 14:28:16,183 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.&lt;/p&gt;

&lt;p&gt;java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 46291115 but unable to find any edit logs containing txid 46291058&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1537)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1495)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:644)&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:379)&lt;/p&gt;

&lt;p&gt;scp slave3:/data1/hadoop/hdfs/name/current/edits_inprogress_0000000000046291058 /tmp/&lt;/p&gt;

&lt;p&gt;sudo su – hdfs&lt;/p&gt;

&lt;p&gt;-bash-4.3$ cp /tmp/edits_inprogress_0000000000046291058 /data1/hadoop/hdfs/name/current/edits_inprogress_0000000000046291058&lt;/p&gt;

&lt;p&gt;-bash-4.3$ ll /data1/hadoop/hdfs/name/current/edits_inprogress_0000000000046291058&lt;/p&gt;

&lt;p&gt;-rw-rw-r– 1 hdfs hdfs 1048576 Jul 23 14:31 /data1/hadoop/hdfs/name/current/edits_inprogress_0000000000046291058&lt;/p&gt;</content><author><name>edyliu</name></author><category term="hadoop" /><summary type="html">Don’t need to sell CDH’s benefits. you should know it before want to migrate 🙂</summary></entry><entry><title type="html">Hive Issue: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</title><link href="http://192.168.1.77:4000/2015-06-hive-issue-failed-execution-error-return-code-1-from-org-apache-hadoop-hive-ql-exec-movetask/" rel="alternate" type="text/html" title="Hive Issue: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask" /><published>2015-06-24T18:01:17-04:00</published><updated>2015-06-24T18:01:17-04:00</updated><id>http://192.168.1.77:4000/hive-issue-failed-execution-error-return-code-1-from-org-apache-hadoop-hive-ql-exec-movetask</id><content type="html" xml:base="http://192.168.1.77:4000/2015-06-hive-issue-failed-execution-error-return-code-1-from-org-apache-hadoop-hive-ql-exec-movetask/">&lt;p&gt;2015-06-25 11:16:52,554 INFO [main]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(623)) – 0: get_table : db=lntest tbl=tmp_download&lt;/p&gt;

&lt;p&gt;2015-06-25 11:16:52,554 INFO [main]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(305)) – ugi=abcuser ip=unknown-ip-addr cmd=get_table : db=lntest tbl=&lt;/p&gt;

&lt;p&gt;tmp_download&lt;/p&gt;

&lt;p&gt;2015-06-25 11:16:52,555 ERROR [main]: bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) – Database access problem. Killing off this connection and all r&lt;/p&gt;

&lt;p&gt;emaining connections in the connection pool. SQL State = 08S01&lt;/p&gt;

&lt;p&gt;2015-06-25 11:16:52,557 ERROR [main]: bonecp.BoneCP (BoneCP.java:destroyConnection(221)) – Error in attempting to close connection&lt;/p&gt;

&lt;p&gt;com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Communications link failure during rollback(). Transaction resolution unknown.&lt;/p&gt;

&lt;p&gt;at sun.reflect.GeneratedConstructorAccessor114.newInstance(Unknown Source)&lt;/p&gt;

&lt;p&gt;Problem related with MySQL for Hive settings:&lt;/p&gt;

&lt;p&gt;mysql&amp;gt; show global variables like ‘%time%’ ;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;update it make it something like 28800 (default)&lt;/p&gt;

&lt;p&gt;mysql&amp;gt; SET GLOBAL wait_timeout = 28800;&lt;/p&gt;

&lt;p&gt;update my.cnf to update it permanent by adding:&lt;/p&gt;

&lt;p&gt;wait_timeout = 28800&lt;/p&gt;

&lt;p&gt;Or you can set interactiveClient=true in the JDBC connection string. Which uses an alternative timeout period.&lt;/p&gt;</content><author><name>edyliu</name></author><category term="bigdata" /><summary type="html">2015-06-25 11:16:52,554 INFO [main]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(623)) – 0: get_table : db=lntest tbl=tmp_download 2015-06-25 11:16:52,554 INFO [main]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(305)) – ugi=abcuser ip=unknown-ip-addr cmd=get_table : db=lntest tbl= tmp_download 2015-06-25 11:16:52,555 ERROR [main]: bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) – Database access problem. Killing off this connection and all r emaining connections in the connection pool. SQL State = 08S01 2015-06-25 11:16:52,557 ERROR [main]: bonecp.BoneCP (BoneCP.java:destroyConnection(221)) – Error in attempting to close connection com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Communications link failure during rollback(). Transaction resolution unknown. at sun.reflect.GeneratedConstructorAccessor114.newInstance(Unknown Source)</summary></entry><entry><title type="html">How to upgrade WAP4400N firmware from 1.2.14 to 1.2.19</title><link href="http://192.168.1.77:4000/2015-05-how-to-upgrade-wap4400n-firmware-from-1-2-14-to-1-2-19/" rel="alternate" type="text/html" title="How to upgrade WAP4400N firmware from 1.2.14 to 1.2.19" /><published>2015-05-17T21:04:06-04:00</published><updated>2015-05-17T21:04:06-04:00</updated><id>http://192.168.1.77:4000/how-to-upgrade-wap4400n-firmware-from-1-2-14-to-1-2-19</id><content type="html" xml:base="http://192.168.1.77:4000/2015-05-how-to-upgrade-wap4400n-firmware-from-1-2-14-to-1-2-19/">&lt;p&gt;Seems it’s a old wireless Router. no new firmware released now.&lt;/p&gt;

&lt;p&gt;Why I want to upgrade firmeware ? just for fun. didn’t feel much improvement. 🙂&lt;/p&gt;

&lt;p&gt;Just got a official link&lt;/p&gt;

&lt;p&gt;http://osdn.jp/projects/sfnet_officiallinksysfirmware/downloads/wap4400n/1.2.19/WAP4400N_GPL_v1.2.19.tar.bz2/&lt;/p&gt;

&lt;p&gt;I had to use an GCC 3.x box. so I choose CentOS 4.8. so old.&lt;/p&gt;

&lt;p&gt;Download firmware from above link, following the cross compile guide. will be straight forward.&lt;/p&gt;

&lt;p&gt;Packages installed: gcc bison flex gcc-c++ zlib-devel&lt;/p&gt;

&lt;p&gt;After a looong compile. got the new firemware 1.2.19.&lt;/p&gt;

&lt;p&gt;Upgrade by your own risk 🙂&lt;/p&gt;

&lt;p&gt;Get the firmware: http://192.168.1.77:8880/soft/wap4400n_v1219.img&lt;/p&gt;</content><author><name>edyliu</name></author><summary type="html">Seems it’s a old wireless Router. no new firmware released now.</summary></entry></feed>